{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "section_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "qaeYIKfLf6fg",
        "outputId": "2b686dec-c77e-4281-e1a3-76c7198ba79a"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f2975be3-5be3-452f-86f5-dcddd26020c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f2975be3-5be3-452f-86f5-dcddd26020c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyUpLNzciYd7"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zJSqkKhgKLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aff096b-e0df-4123-aafd-b9f7594461f5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "sents = pd.read_csv('1000sents.csv', error_bad_lines=False)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 676: expected 12 fields, saw 13\\nSkipping line 764: expected 12 fields, saw 13\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeNKWWUzgQ6M"
      },
      "source": [
        "sents = sents[['HEADWORD','JAPANESE','EXAMPLE (KO)','EXAMPLE (JA)']]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAqQ1wtZlr2j"
      },
      "source": [
        "kr, jp = sents['EXAMPLE (KO)'], sents['EXAMPLE (JA)']"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyuPVcx_l6kd",
        "outputId": "2c889625-3ce6-45a9-a030-a077f3cf613a"
      },
      "source": [
        "sents.drop(columns=['EXAMPLE (KO)','EXAMPLE (JA)'], inplace=True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zw1KEsymCf4"
      },
      "source": [
        "a = pd.concat([kr,jp], axis=1)\n",
        "a.columns = ['HEADWORD', 'JAPANESE']"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FehiXp-imUxy"
      },
      "source": [
        "sents = pd.concat([sents, a], axis=0)\n",
        "sents.columns = ['eng','jp']"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "0sSaccgxU9al",
        "outputId": "f7ff6383-5d75-4677-8b85-628354907d17"
      },
      "source": [
        "sents"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>jp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>가게</td>\n",
              "      <td>店</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>가격</td>\n",
              "      <td>価格, 値段</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>가깝다</td>\n",
              "      <td>近い</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>가끔</td>\n",
              "      <td>たまに</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가능하다</td>\n",
              "      <td>可能だ, できる</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>휴지는 휴지통에 버리세요.</td>\n",
              "      <td>紙くずはごみ箱に捨ててください。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>우리 집 근처에 작은 강이 흘러요.</td>\n",
              "      <td>我家の近くに小さな川が流れてます。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>흰색이 니 까만 피부랑 잘 어울려.</td>\n",
              "      <td>白がお前の黒い肌によく似合う。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>오늘은 테니스 칠 힘이 없어.</td>\n",
              "      <td>今日はテニスやる力がない。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>요즘은 취직하기가 너무 힘들어.</td>\n",
              "      <td>このごろは就職するのがとても大変なの。</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1996 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     eng                    jp\n",
              "0                     가게                     店\n",
              "1                     가격                価格, 値段\n",
              "2                    가깝다                    近い\n",
              "3                     가끔                   たまに\n",
              "4                   가능하다              可能だ, できる\n",
              "..                   ...                   ...\n",
              "993      휴지는 휴지통에 버리세요.      紙くずはごみ箱に捨ててください。 \n",
              "994  우리 집 근처에 작은 강이 흘러요.     我家の近くに小さな川が流れてます。\n",
              "995  흰색이 니 까만 피부랑 잘 어울려.       白がお前の黒い肌によく似合う。\n",
              "996     오늘은 테니스 칠 힘이 없어.         今日はテニスやる力がない。\n",
              "997   요즘은 취직하기가 너무 힘들어.   このごろは就職するのがとても大変なの。 \n",
              "\n",
              "[1996 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcxIKGAjh3cp"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFC', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP--lXGqh4RP"
      },
      "source": [
        "def preprocess_sentence(sent):\n",
        "    # 위에서 구현한 함수를 내부적으로 호출\n",
        "    sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "    sent = re.sub(r\"[^가-힇ㄱ-ㅎㅏ-ㅣぁ-ゔァ-ヴー々〆〤一-龥!.?。]+\", r\" \", sent)\n",
        "\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    return sent"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4Uzeq5iAqs",
        "outputId": "2c09e188-2022-4db7-cbc6-6251e7a63500"
      },
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"저녁 먹었어요?\"\n",
        "jp_sent = u\"ご飯食べたか。\"\n",
        "print(preprocess_sentence(en_sent))\n",
        "print(preprocess_sentence(jp_sent))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "저녁 먹었어요 ?\n",
            "ご飯食べたか。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDgeRV5igWXv",
        "outputId": "e54558f2-829f-4a76-94f1-cba6c28d00fa"
      },
      "source": [
        "!pip install sentencepiece==0.1.83"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.83\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/cf/7089b87fdae8f47be81ce8e2e6377b321805c4648f2eb12fbd2987388dac/sentencepiece-0.1.83-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 11.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kihNW3TNgke-"
      },
      "source": [
        "with open('english.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(sents['eng']))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uOaGn46gmeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc691f9c-965f-42ed-db01-a85761ed2fb2"
      },
      "source": [
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.Train('--input=english.txt --model_prefix=english_train --vocab_size=32000 --model_type=unigram --max_sentence_length=9999 --character_coverage=1.0 --hard_vocab_limit=false')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7RN0XSgwC1",
        "outputId": "8bd721e7-a0d4-4a1c-978b-7be35c071cea"
      },
      "source": [
        "import sentencepiece as spm\n",
        "sp_en = spm.SentencePieceProcessor()\n",
        "vocab_file = \"english_train.model\"\n",
        "sp_en.load(vocab_file)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkJkVQ4Eg0N5"
      },
      "source": [
        "with open('japanese.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(sents['jp']))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hTh2_xg1lw",
        "outputId": "57357bc5-2c67-4b2d-bff4-827548120e0c"
      },
      "source": [
        "spm.SentencePieceTrainer.Train('--input=japanese.txt --model_prefix=japanese_train --vocab_size=32000 --model_type=unigram --character_coverage=0.9995 --max_sentence_length=9999 --hard_vocab_limit=false')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNb-EXjCg9AC",
        "outputId": "9d0b9e9f-d3c9-44aa-e68c-fff42b8641ec"
      },
      "source": [
        "sp_jp = spm.SentencePieceProcessor()\n",
        "vocab_file = \"japanese_train.model\"\n",
        "sp_jp.load(vocab_file)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqmM3xiChATw",
        "outputId": "e3976397-9b45-441b-9d9e-fd753cc87557"
      },
      "source": [
        "sp_en.get_piece_size(), sp_jp.get_piece_size()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2225, 3509)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QmwpjGdhEQ1"
      },
      "source": [
        "# START_TOKEN, END_TOKEN = [sp_en.get_piece_size()], [sp_en.get_piece_size() + 1]\n",
        "# START_TOKEN_J, END_TOKEN_J = [sp_jp.get_piece_size()], [sp_jp.get_piece_size() + 1]\n",
        "# # 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
        "# VOCAB_SIZE = sp_en.get_piece_size() + 2\n",
        "# VOCAB_SIZE_J= sp_jp.get_piece_size() + 2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLJR3z5wPOd5"
      },
      "source": [
        "num_samples = 5000"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bzs6RSkinCa"
      },
      "source": [
        "def load_preprocessed_data():\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "    i = 0\n",
        "    sos = [1]\n",
        "    eos = [2]\n",
        "    for kr, jp in zip(sents['eng'], sents['jp']) :\n",
        "        src_line, tar_line = kr, jp\n",
        "        src_line_input = [w for w in sp_en.EncodeAsIds(preprocess_sentence(src_line))]\n",
        "\n",
        "        # # target 데이터 전처리\n",
        "        tar_line_input = sos + (sp_jp.EncodeAsIds(preprocess_sentence(tar_line)))\n",
        "        tar_line_target = (sp_jp.EncodeAsIds(preprocess_sentence(tar_line))) + eos\n",
        "        encoder_input.append(src_line_input)\n",
        "        decoder_input.append(tar_line_input)\n",
        "        decoder_target.append(tar_line_target)\n",
        "        i += 1\n",
        "        if i == num_samples - 1:\n",
        "                break\n",
        "    return encoder_input, decoder_input, decoder_target"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL9lf0wpjMT0",
        "outputId": "8a70c68c-3269-4878-896f-ebe500898585"
      },
      "source": [
        "encoder_input, decoder_input, decoder_target = load_preprocessed_data()\n",
        "print(encoder_input[:5])\n",
        "print(decoder_input[:5])\n",
        "print(decoder_target[:5])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[160], [747], [983], [1290], [1186]]\n",
            "[[1, 1610], [1, 1566, 3, 483], [1, 3, 1403], [1, 3, 1535], [1, 3, 1109, 844, 14, 1630]]\n",
            "[[1610, 2], [1566, 3, 483, 2], [3, 1403, 2], [3, 1535, 2], [3, 1109, 844, 14, 1630, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYZ-JK9Ps5vb"
      },
      "source": [
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF9fDySqs7gg",
        "outputId": "a0744264-0808-4d49-e534-4040d68fcbb9"
      },
      "source": [
        "src_vocab_size = sp_en.get_piece_size() + 1\n",
        "tar_vocab_size = sp_jp.get_piece_size() + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 일어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 단어 집합의 크기 : 2226, 일어 단어 집합의 크기 : 3510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j6dVgZNtLa8",
        "outputId": "f91a319d-0197-4cc0-af17-71c2a8d60ec9"
      },
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 981  957  149 ... 1063 1081 1509]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwhiKbb5u4zJ"
      },
      "source": [
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0uir1_8wezx",
        "outputId": "883b4543-566d-47c1-8a28-d1108143db89"
      },
      "source": [
        "encoder_input.shape\n",
        "decoder_input.shape"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1996, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BART1Hqnu5_3",
        "outputId": "afb300ed-701d-4d2a-f55a-17d2f32284b4"
      },
      "source": [
        "n_of_val = int(2000*0.1)\n",
        "print(n_of_val)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JACn7sEiGPXU"
      },
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhbgk4pZGTC5"
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4w_1wBiGUjz"
      },
      "source": [
        "latent_dim = 50"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A29ox6mfGV5K"
      },
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJ9ppC2GXKv"
      },
      "source": [
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eieuIFwxGYZY"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFUW0DMQGZv1"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdgz97utGa5D",
        "outputId": "2ce83f25-7944-46e1-ee91-5c39067e7377"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 50)     111300      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 50)     175500      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "masking_2 (Masking)             (None, None, 50)     0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "masking_3 (Masking)             (None, None, 50)     0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50), (None,  20200       masking_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       masking_3[0][0]                  \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 3510)   179010      lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 506,210\n",
            "Trainable params: 506,210\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7YwXZNHROt3"
      },
      "source": [
        "import datetime\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "checkpoint_filepath = \"seq_best.hdf5\"\n",
        "save_best = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', save_freq='epoch', options=None)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=10)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSaAuCyqGvGt"
      },
      "source": [
        "# !pip install wandb"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVT2EWMyG73-"
      },
      "source": [
        "# !wandb login eb738ba304b560fc8b41bf28ba686d71a2b3d5d4"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PWcQ2AaGySS"
      },
      "source": [
        "# import wandb\n",
        "# from wandb.keras import WandbCallback"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSICykddG1a6"
      },
      "source": [
        "# wandb_project = \"seq2\"\n",
        "# wandb_group = \"\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfEEVfYeGcRY",
        "outputId": "e5a42573-2d67-4d03-8f79-8dc3cbc6ab05"
      },
      "source": [
        "#wandb.init(project=wandb_project) \n",
        "\n",
        "#wandb.config.epochs = 50\n",
        "#wandb.config.batch_size = 16\n",
        "\n",
        "\n",
        "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size =24, epochs = 200, callbacks=[save_best])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "75/75 [==============================] - 12s 52ms/step - loss: 5.8966 - acc: 0.6643 - val_loss: 2.3525 - val_acc: 0.7010\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.70100, saving model to seq_best.hdf5\n",
            "Epoch 2/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 2.0965 - acc: 0.7093 - val_loss: 1.9423 - val_acc: 0.7050\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.70100 to 0.70500, saving model to seq_best.hdf5\n",
            "Epoch 3/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.7444 - acc: 0.7286 - val_loss: 1.8232 - val_acc: 0.7542\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.70500 to 0.75425, saving model to seq_best.hdf5\n",
            "Epoch 4/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.6437 - acc: 0.7702 - val_loss: 1.7907 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.75425 to 0.76825, saving model to seq_best.hdf5\n",
            "Epoch 5/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.5512 - acc: 0.7831 - val_loss: 1.8224 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.76825 to 0.77425, saving model to seq_best.hdf5\n",
            "Epoch 6/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.5500 - acc: 0.7876 - val_loss: 1.7974 - val_acc: 0.7803\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.77425 to 0.78025, saving model to seq_best.hdf5\n",
            "Epoch 7/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.5025 - acc: 0.7928 - val_loss: 1.7441 - val_acc: 0.7900\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.78025 to 0.79000, saving model to seq_best.hdf5\n",
            "Epoch 8/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.4658 - acc: 0.7984 - val_loss: 1.7414 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.79000 to 0.79175, saving model to seq_best.hdf5\n",
            "Epoch 9/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.4761 - acc: 0.7953 - val_loss: 1.7457 - val_acc: 0.7928\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.79175 to 0.79275, saving model to seq_best.hdf5\n",
            "Epoch 10/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.4529 - acc: 0.7970 - val_loss: 1.7465 - val_acc: 0.7947\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.79275 to 0.79475, saving model to seq_best.hdf5\n",
            "Epoch 11/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.4072 - acc: 0.8028 - val_loss: 1.7632 - val_acc: 0.7958\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.79475 to 0.79575, saving model to seq_best.hdf5\n",
            "Epoch 12/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3725 - acc: 0.8066 - val_loss: 1.7507 - val_acc: 0.7952\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.79575\n",
            "Epoch 13/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.4069 - acc: 0.8008 - val_loss: 1.7788 - val_acc: 0.7947\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.79575\n",
            "Epoch 14/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3719 - acc: 0.8040 - val_loss: 1.7809 - val_acc: 0.7960\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.79575 to 0.79600, saving model to seq_best.hdf5\n",
            "Epoch 15/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.3515 - acc: 0.8062 - val_loss: 1.7844 - val_acc: 0.7962\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.79600 to 0.79625, saving model to seq_best.hdf5\n",
            "Epoch 16/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.3579 - acc: 0.8050 - val_loss: 1.7661 - val_acc: 0.7945\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.79625\n",
            "Epoch 17/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.3626 - acc: 0.8017 - val_loss: 1.7953 - val_acc: 0.7937\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.79625\n",
            "Epoch 18/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3466 - acc: 0.8032 - val_loss: 1.7743 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.79625\n",
            "Epoch 19/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3086 - acc: 0.8090 - val_loss: 1.7980 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.79625\n",
            "Epoch 20/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.3017 - acc: 0.8084 - val_loss: 1.7908 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.79625\n",
            "Epoch 21/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3382 - acc: 0.8020 - val_loss: 1.7999 - val_acc: 0.7952\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.79625\n",
            "Epoch 22/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2893 - acc: 0.8079 - val_loss: 1.8161 - val_acc: 0.7912\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.79625\n",
            "Epoch 23/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.3074 - acc: 0.8044 - val_loss: 1.8135 - val_acc: 0.7943\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.79625\n",
            "Epoch 24/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2787 - acc: 0.8088 - val_loss: 1.8203 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.79625\n",
            "Epoch 25/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.2765 - acc: 0.8073 - val_loss: 1.8452 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.79625\n",
            "Epoch 26/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2560 - acc: 0.8095 - val_loss: 1.8346 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.79625\n",
            "Epoch 27/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2368 - acc: 0.8118 - val_loss: 1.8330 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.79625\n",
            "Epoch 28/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.2033 - acc: 0.8162 - val_loss: 1.8656 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.79625\n",
            "Epoch 29/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2328 - acc: 0.8106 - val_loss: 1.8448 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.79625\n",
            "Epoch 30/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2053 - acc: 0.8146 - val_loss: 1.8673 - val_acc: 0.7940\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.79625\n",
            "Epoch 31/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1804 - acc: 0.8170 - val_loss: 1.8564 - val_acc: 0.7908\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.79625\n",
            "Epoch 32/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1960 - acc: 0.8130 - val_loss: 1.8662 - val_acc: 0.7905\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.79625\n",
            "Epoch 33/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.2386 - acc: 0.8065 - val_loss: 1.8799 - val_acc: 0.7910\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.79625\n",
            "Epoch 34/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1653 - acc: 0.8155 - val_loss: 1.8796 - val_acc: 0.7900\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.79625\n",
            "Epoch 35/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1586 - acc: 0.8167 - val_loss: 1.9362 - val_acc: 0.7910\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.79625\n",
            "Epoch 36/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.1624 - acc: 0.8155 - val_loss: 1.9248 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.79625\n",
            "Epoch 37/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1650 - acc: 0.8143 - val_loss: 1.9188 - val_acc: 0.7918\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.79625\n",
            "Epoch 38/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1058 - acc: 0.8229 - val_loss: 1.9313 - val_acc: 0.7887\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.79625\n",
            "Epoch 39/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.1060 - acc: 0.8216 - val_loss: 1.9437 - val_acc: 0.7862\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.79625\n",
            "Epoch 40/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.1336 - acc: 0.8164 - val_loss: 1.9275 - val_acc: 0.7865\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.79625\n",
            "Epoch 41/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.1086 - acc: 0.8194 - val_loss: 1.9424 - val_acc: 0.7880\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.79625\n",
            "Epoch 42/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.0962 - acc: 0.8214 - val_loss: 1.9574 - val_acc: 0.7855\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.79625\n",
            "Epoch 43/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0868 - acc: 0.8206 - val_loss: 1.9519 - val_acc: 0.7850\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.79625\n",
            "Epoch 44/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0879 - acc: 0.8206 - val_loss: 1.9598 - val_acc: 0.7862\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.79625\n",
            "Epoch 45/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0551 - acc: 0.8254 - val_loss: 1.9848 - val_acc: 0.7875\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.79625\n",
            "Epoch 46/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0534 - acc: 0.8234 - val_loss: 1.9837 - val_acc: 0.7868\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.79625\n",
            "Epoch 47/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0365 - acc: 0.8277 - val_loss: 1.9966 - val_acc: 0.7878\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.79625\n",
            "Epoch 48/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0469 - acc: 0.8242 - val_loss: 1.9946 - val_acc: 0.7878\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.79625\n",
            "Epoch 49/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0772 - acc: 0.8190 - val_loss: 2.0173 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.79625\n",
            "Epoch 50/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0437 - acc: 0.8237 - val_loss: 2.0317 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.79625\n",
            "Epoch 51/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0368 - acc: 0.8252 - val_loss: 2.0251 - val_acc: 0.7832\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.79625\n",
            "Epoch 52/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0554 - acc: 0.8211 - val_loss: 2.0378 - val_acc: 0.7872\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.79625\n",
            "Epoch 53/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0327 - acc: 0.8248 - val_loss: 2.0356 - val_acc: 0.7850\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.79625\n",
            "Epoch 54/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0203 - acc: 0.8265 - val_loss: 2.0469 - val_acc: 0.7845\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.79625\n",
            "Epoch 55/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 1.0010 - acc: 0.8285 - val_loss: 2.0587 - val_acc: 0.7828\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.79625\n",
            "Epoch 56/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9782 - acc: 0.8320 - val_loss: 2.0621 - val_acc: 0.7845\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.79625\n",
            "Epoch 57/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9767 - acc: 0.8322 - val_loss: 2.0561 - val_acc: 0.7845\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.79625\n",
            "Epoch 58/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9944 - acc: 0.8281 - val_loss: 2.0629 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.79625\n",
            "Epoch 59/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9859 - acc: 0.8304 - val_loss: 2.0796 - val_acc: 0.7828\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.79625\n",
            "Epoch 60/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9758 - acc: 0.8307 - val_loss: 2.0826 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.79625\n",
            "Epoch 61/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9190 - acc: 0.8405 - val_loss: 2.0897 - val_acc: 0.7857\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.79625\n",
            "Epoch 62/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9650 - acc: 0.8308 - val_loss: 2.0934 - val_acc: 0.7840\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.79625\n",
            "Epoch 63/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9643 - acc: 0.8312 - val_loss: 2.0938 - val_acc: 0.7850\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.79625\n",
            "Epoch 64/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9262 - acc: 0.8383 - val_loss: 2.1076 - val_acc: 0.7840\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.79625\n",
            "Epoch 65/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9096 - acc: 0.8414 - val_loss: 2.1075 - val_acc: 0.7828\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.79625\n",
            "Epoch 66/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.9268 - acc: 0.8371 - val_loss: 2.1143 - val_acc: 0.7818\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.79625\n",
            "Epoch 67/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.9395 - acc: 0.8348 - val_loss: 2.1093 - val_acc: 0.7815\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.79625\n",
            "Epoch 68/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9353 - acc: 0.8342 - val_loss: 2.1344 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.79625\n",
            "Epoch 69/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8950 - acc: 0.8403 - val_loss: 2.1248 - val_acc: 0.7825\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.79625\n",
            "Epoch 70/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8848 - acc: 0.8429 - val_loss: 2.1297 - val_acc: 0.7803\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.79625\n",
            "Epoch 71/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.9014 - acc: 0.8400 - val_loss: 2.1360 - val_acc: 0.7810\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.79625\n",
            "Epoch 72/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.8752 - acc: 0.8444 - val_loss: 2.1432 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.79625\n",
            "Epoch 73/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8746 - acc: 0.8438 - val_loss: 2.1471 - val_acc: 0.7822\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.79625\n",
            "Epoch 74/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.8560 - acc: 0.8489 - val_loss: 2.1657 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.79625\n",
            "Epoch 75/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.8646 - acc: 0.8470 - val_loss: 2.1843 - val_acc: 0.7818\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.79625\n",
            "Epoch 76/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8725 - acc: 0.8436 - val_loss: 2.1488 - val_acc: 0.7815\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.79625\n",
            "Epoch 77/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8182 - acc: 0.8531 - val_loss: 2.1582 - val_acc: 0.7835\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.79625\n",
            "Epoch 78/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.8528 - acc: 0.8467 - val_loss: 2.1715 - val_acc: 0.7793\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.79625\n",
            "Epoch 79/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8390 - acc: 0.8500 - val_loss: 2.1757 - val_acc: 0.7825\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.79625\n",
            "Epoch 80/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.8142 - acc: 0.8533 - val_loss: 2.1653 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.79625\n",
            "Epoch 81/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8265 - acc: 0.8512 - val_loss: 2.1928 - val_acc: 0.7815\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.79625\n",
            "Epoch 82/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8104 - acc: 0.8545 - val_loss: 2.1873 - val_acc: 0.7822\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.79625\n",
            "Epoch 83/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8038 - acc: 0.8564 - val_loss: 2.2036 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.79625\n",
            "Epoch 84/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8350 - acc: 0.8506 - val_loss: 2.1827 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.79625\n",
            "Epoch 85/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7924 - acc: 0.8586 - val_loss: 2.2002 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.79625\n",
            "Epoch 86/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.8259 - acc: 0.8501 - val_loss: 2.2077 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.79625\n",
            "Epoch 87/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7834 - acc: 0.8597 - val_loss: 2.1897 - val_acc: 0.7788\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.79625\n",
            "Epoch 88/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7987 - acc: 0.8557 - val_loss: 2.2072 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.79625\n",
            "Epoch 89/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7781 - acc: 0.8612 - val_loss: 2.2170 - val_acc: 0.7790\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.79625\n",
            "Epoch 90/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7641 - acc: 0.8642 - val_loss: 2.2147 - val_acc: 0.7785\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.79625\n",
            "Epoch 91/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.7793 - acc: 0.8605 - val_loss: 2.2420 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.79625\n",
            "Epoch 92/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7540 - acc: 0.8658 - val_loss: 2.2187 - val_acc: 0.7788\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.79625\n",
            "Epoch 93/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7300 - acc: 0.8692 - val_loss: 2.2453 - val_acc: 0.7797\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.79625\n",
            "Epoch 94/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7614 - acc: 0.8639 - val_loss: 2.2619 - val_acc: 0.7797\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.79625\n",
            "Epoch 95/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7605 - acc: 0.8650 - val_loss: 2.2574 - val_acc: 0.7785\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.79625\n",
            "Epoch 96/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7574 - acc: 0.8660 - val_loss: 2.2517 - val_acc: 0.7772\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.79625\n",
            "Epoch 97/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7267 - acc: 0.8707 - val_loss: 2.2701 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.79625\n",
            "Epoch 98/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7360 - acc: 0.8703 - val_loss: 2.2515 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.79625\n",
            "Epoch 99/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7166 - acc: 0.8725 - val_loss: 2.2710 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.79625\n",
            "Epoch 100/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7272 - acc: 0.8724 - val_loss: 2.2826 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.79625\n",
            "Epoch 101/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.7244 - acc: 0.8726 - val_loss: 2.2637 - val_acc: 0.7782\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.79625\n",
            "Epoch 102/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7120 - acc: 0.8745 - val_loss: 2.2879 - val_acc: 0.7770\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.79625\n",
            "Epoch 103/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7256 - acc: 0.8715 - val_loss: 2.2820 - val_acc: 0.7778\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.79625\n",
            "Epoch 104/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7016 - acc: 0.8773 - val_loss: 2.3063 - val_acc: 0.7780\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.79625\n",
            "Epoch 105/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6965 - acc: 0.8788 - val_loss: 2.3148 - val_acc: 0.7770\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.79625\n",
            "Epoch 106/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6785 - acc: 0.8809 - val_loss: 2.2920 - val_acc: 0.7775\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.79625\n",
            "Epoch 107/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.7161 - acc: 0.8753 - val_loss: 2.3125 - val_acc: 0.7770\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.79625\n",
            "Epoch 108/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6865 - acc: 0.8811 - val_loss: 2.3353 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.79625\n",
            "Epoch 109/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6777 - acc: 0.8842 - val_loss: 2.3208 - val_acc: 0.7778\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.79625\n",
            "Epoch 110/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6771 - acc: 0.8830 - val_loss: 2.3379 - val_acc: 0.7768\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.79625\n",
            "Epoch 111/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6675 - acc: 0.8845 - val_loss: 2.3321 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.79625\n",
            "Epoch 112/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6707 - acc: 0.8840 - val_loss: 2.3385 - val_acc: 0.7732\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.79625\n",
            "Epoch 113/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6433 - acc: 0.8898 - val_loss: 2.3357 - val_acc: 0.7747\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.79625\n",
            "Epoch 114/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6546 - acc: 0.8882 - val_loss: 2.3469 - val_acc: 0.7745\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.79625\n",
            "Epoch 115/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6602 - acc: 0.8868 - val_loss: 2.3466 - val_acc: 0.7757\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.79625\n",
            "Epoch 116/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6453 - acc: 0.8909 - val_loss: 2.3616 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.79625\n",
            "Epoch 117/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6342 - acc: 0.8939 - val_loss: 2.3704 - val_acc: 0.7720\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.79625\n",
            "Epoch 118/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6440 - acc: 0.8923 - val_loss: 2.3742 - val_acc: 0.7745\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.79625\n",
            "Epoch 119/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6293 - acc: 0.8957 - val_loss: 2.3859 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.79625\n",
            "Epoch 120/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6078 - acc: 0.8989 - val_loss: 2.3699 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.79625\n",
            "Epoch 121/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6204 - acc: 0.8992 - val_loss: 2.3740 - val_acc: 0.7745\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.79625\n",
            "Epoch 122/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5972 - acc: 0.9011 - val_loss: 2.3890 - val_acc: 0.7738\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.79625\n",
            "Epoch 123/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6139 - acc: 0.8994 - val_loss: 2.3970 - val_acc: 0.7732\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.79625\n",
            "Epoch 124/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.6015 - acc: 0.9014 - val_loss: 2.3929 - val_acc: 0.7732\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.79625\n",
            "Epoch 125/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5976 - acc: 0.9026 - val_loss: 2.3931 - val_acc: 0.7742\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.79625\n",
            "Epoch 126/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.6089 - acc: 0.9018 - val_loss: 2.4032 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.79625\n",
            "Epoch 127/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5960 - acc: 0.9023 - val_loss: 2.4093 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.79625\n",
            "Epoch 128/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5840 - acc: 0.9077 - val_loss: 2.4213 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.79625\n",
            "Epoch 129/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5819 - acc: 0.9056 - val_loss: 2.4205 - val_acc: 0.7738\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.79625\n",
            "Epoch 130/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5674 - acc: 0.9091 - val_loss: 2.4186 - val_acc: 0.7725\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.79625\n",
            "Epoch 131/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5814 - acc: 0.9067 - val_loss: 2.4324 - val_acc: 0.7738\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.79625\n",
            "Epoch 132/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5696 - acc: 0.9100 - val_loss: 2.4428 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.79625\n",
            "Epoch 133/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5722 - acc: 0.9110 - val_loss: 2.4262 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.79625\n",
            "Epoch 134/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5667 - acc: 0.9113 - val_loss: 2.4632 - val_acc: 0.7740\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.79625\n",
            "Epoch 135/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5639 - acc: 0.9112 - val_loss: 2.4509 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.79625\n",
            "Epoch 136/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5474 - acc: 0.9134 - val_loss: 2.4574 - val_acc: 0.7745\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.79625\n",
            "Epoch 137/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5472 - acc: 0.9159 - val_loss: 2.4569 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.79625\n",
            "Epoch 138/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5494 - acc: 0.9152 - val_loss: 2.4474 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.79625\n",
            "Epoch 139/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.5379 - acc: 0.9177 - val_loss: 2.4730 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.79625\n",
            "Epoch 140/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5208 - acc: 0.9184 - val_loss: 2.4621 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.79625\n",
            "Epoch 141/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5397 - acc: 0.9160 - val_loss: 2.4787 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.79625\n",
            "Epoch 142/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5297 - acc: 0.9196 - val_loss: 2.4685 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.79625\n",
            "Epoch 143/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5203 - acc: 0.9192 - val_loss: 2.4628 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.79625\n",
            "Epoch 144/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5167 - acc: 0.9207 - val_loss: 2.4923 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.79625\n",
            "Epoch 145/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5142 - acc: 0.9205 - val_loss: 2.5018 - val_acc: 0.7713\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.79625\n",
            "Epoch 146/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5033 - acc: 0.9241 - val_loss: 2.4918 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.79625\n",
            "Epoch 147/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4946 - acc: 0.9259 - val_loss: 2.4930 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.79625\n",
            "Epoch 148/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4881 - acc: 0.9243 - val_loss: 2.4831 - val_acc: 0.7713\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.79625\n",
            "Epoch 149/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4936 - acc: 0.9246 - val_loss: 2.4947 - val_acc: 0.7713\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.79625\n",
            "Epoch 150/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.5025 - acc: 0.9239 - val_loss: 2.5064 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.79625\n",
            "Epoch 151/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4875 - acc: 0.9265 - val_loss: 2.4984 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.79625\n",
            "Epoch 152/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4765 - acc: 0.9286 - val_loss: 2.5085 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.79625\n",
            "Epoch 153/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4833 - acc: 0.9267 - val_loss: 2.5101 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.79625\n",
            "Epoch 154/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4828 - acc: 0.9301 - val_loss: 2.5123 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.79625\n",
            "Epoch 155/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4857 - acc: 0.9259 - val_loss: 2.5109 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.79625\n",
            "Epoch 156/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4655 - acc: 0.9320 - val_loss: 2.5130 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.79625\n",
            "Epoch 157/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4636 - acc: 0.9307 - val_loss: 2.5185 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.79625\n",
            "Epoch 158/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4558 - acc: 0.9330 - val_loss: 2.5312 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.79625\n",
            "Epoch 159/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4556 - acc: 0.9318 - val_loss: 2.5349 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.79625\n",
            "Epoch 160/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4501 - acc: 0.9341 - val_loss: 2.5318 - val_acc: 0.7715\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.79625\n",
            "Epoch 161/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4462 - acc: 0.9334 - val_loss: 2.5503 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.79625\n",
            "Epoch 162/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4457 - acc: 0.9332 - val_loss: 2.5510 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.79625\n",
            "Epoch 163/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4342 - acc: 0.9350 - val_loss: 2.5567 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.79625\n",
            "Epoch 164/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4394 - acc: 0.9363 - val_loss: 2.5599 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.79625\n",
            "Epoch 165/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4377 - acc: 0.9359 - val_loss: 2.5510 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.79625\n",
            "Epoch 166/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.4334 - acc: 0.9378 - val_loss: 2.5642 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.79625\n",
            "Epoch 167/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4374 - acc: 0.9370 - val_loss: 2.5561 - val_acc: 0.7713\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.79625\n",
            "Epoch 168/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4353 - acc: 0.9358 - val_loss: 2.5638 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.79625\n",
            "Epoch 169/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4215 - acc: 0.9390 - val_loss: 2.5757 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.79625\n",
            "Epoch 170/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4184 - acc: 0.9396 - val_loss: 2.5792 - val_acc: 0.7703\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.79625\n",
            "Epoch 171/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4165 - acc: 0.9390 - val_loss: 2.5819 - val_acc: 0.7688\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.79625\n",
            "Epoch 172/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4167 - acc: 0.9388 - val_loss: 2.5789 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.79625\n",
            "Epoch 173/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4130 - acc: 0.9400 - val_loss: 2.5999 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.79625\n",
            "Epoch 174/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4039 - acc: 0.9420 - val_loss: 2.5946 - val_acc: 0.7705\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.79625\n",
            "Epoch 175/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4081 - acc: 0.9411 - val_loss: 2.5926 - val_acc: 0.7697\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.79625\n",
            "Epoch 176/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.4012 - acc: 0.9418 - val_loss: 2.5901 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.79625\n",
            "Epoch 177/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3953 - acc: 0.9438 - val_loss: 2.6138 - val_acc: 0.7685\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.79625\n",
            "Epoch 178/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3918 - acc: 0.9438 - val_loss: 2.6080 - val_acc: 0.7688\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.79625\n",
            "Epoch 179/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3882 - acc: 0.9438 - val_loss: 2.6085 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.79625\n",
            "Epoch 180/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3948 - acc: 0.9433 - val_loss: 2.6124 - val_acc: 0.7700\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.79625\n",
            "Epoch 181/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3840 - acc: 0.9450 - val_loss: 2.6169 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.79625\n",
            "Epoch 182/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3839 - acc: 0.9452 - val_loss: 2.6150 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.79625\n",
            "Epoch 183/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3855 - acc: 0.9452 - val_loss: 2.6314 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.79625\n",
            "Epoch 184/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3722 - acc: 0.9468 - val_loss: 2.6268 - val_acc: 0.7685\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.79625\n",
            "Epoch 185/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3747 - acc: 0.9457 - val_loss: 2.6230 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.79625\n",
            "Epoch 186/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3678 - acc: 0.9460 - val_loss: 2.6328 - val_acc: 0.7680\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.79625\n",
            "Epoch 187/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3741 - acc: 0.9463 - val_loss: 2.6399 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.79625\n",
            "Epoch 188/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3608 - acc: 0.9479 - val_loss: 2.6387 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.79625\n",
            "Epoch 189/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3661 - acc: 0.9479 - val_loss: 2.6300 - val_acc: 0.7680\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.79625\n",
            "Epoch 190/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3611 - acc: 0.9479 - val_loss: 2.6428 - val_acc: 0.7655\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.79625\n",
            "Epoch 191/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3515 - acc: 0.9485 - val_loss: 2.6427 - val_acc: 0.7663\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.79625\n",
            "Epoch 192/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3605 - acc: 0.9470 - val_loss: 2.6427 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.79625\n",
            "Epoch 193/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3564 - acc: 0.9499 - val_loss: 2.6435 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.79625\n",
            "Epoch 194/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3466 - acc: 0.9497 - val_loss: 2.6506 - val_acc: 0.7690\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.79625\n",
            "Epoch 195/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3466 - acc: 0.9506 - val_loss: 2.6557 - val_acc: 0.7678\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.79625\n",
            "Epoch 196/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3450 - acc: 0.9501 - val_loss: 2.6601 - val_acc: 0.7667\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.79625\n",
            "Epoch 197/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3405 - acc: 0.9505 - val_loss: 2.6630 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.79625\n",
            "Epoch 198/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3444 - acc: 0.9502 - val_loss: 2.6665 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.79625\n",
            "Epoch 199/200\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.3338 - acc: 0.9521 - val_loss: 2.6696 - val_acc: 0.7675\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.79625\n",
            "Epoch 200/200\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 0.3351 - acc: 0.9516 - val_loss: 2.6655 - val_acc: 0.7667\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.79625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8484f3cfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6gktOEuGjEK"
      },
      "source": [
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VrwIWgyGmqy"
      },
      "source": [
        "# 디코더\n",
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4wUpxIVGoN4"
      },
      "source": [
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnE7VhBVGqCb"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 정수 생성\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sp_jp.piece_to_id('<s>')\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        # 예측 결과를 단어로 변환\n",
        "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
        "        sampled_char = sp_jp.IdToPiece(sampled_token_index)\n",
        "\n",
        "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "        if (sampled_char == '</s>' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA1FHbmdGrar"
      },
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2src(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            temp = temp + sp_en.id_to_piece(int(i))+' '\n",
        "    return temp\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq2tar(input_seq):\n",
        "    temp=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=sp_jp.piece_to_id('<s>')) and i!=sp_jp.piece_to_id('</s>')):\n",
        "            temp = temp +  sp_jp.id_to_piece(int(i)) + ' '\n",
        "    return temp"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co_BYqtpGs3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c4eabd-bd3b-4dff-dc32-2c81f2f8d6dd"
      },
      "source": [
        "for seq_index in [3,50,100,300,1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
        "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
        "  print(\"예측문 :\",decoded_sentence[:-5])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 :  ▁가깝다 \n",
            "번역문 : ▁ 近い \n",
            "예측문 :  ▁ サッカー\n",
            "\n",
            "\n",
            "원문 :  ▁가지 \n",
            "번역문 : ▁ 種類 \n",
            "예측문 :  ▁ 種類\n",
            "\n",
            "\n",
            "원문 :  ▁우리 는 ▁어 렸 을 ▁때 ▁같은 ▁학교에 ▁ 다 녔 다 ▁ . \n",
            "번역문 : ▁私たちは 幼い頃 同じ 学校に 通 った 。 \n",
            "예측문 :  ▁私たちは 幼い頃 同じ 学校に 通 った 。\n",
            "\n",
            "\n",
            "원문 :  ▁우리 는 ▁밤새 ▁춤 추 고 ▁노래 했다 ▁ . \n",
            "번역문 : ▁私たちは 一晩中 歌 って 踊 った 。 \n",
            "예측문 :  ▁ 一晩中 に お仕舞い に ▁彼は なる だ 。\n",
            "\n",
            "\n",
            "원문 :  ▁복잡하 다 \n",
            "번역문 : ▁ 複雑 だ \n",
            "예측문 :  ▁ ない ▁ 個\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spEAolEyqAVx",
        "outputId": "aca9590e-eed3-4261-db3a-f6e9e540cfb9"
      },
      "source": [
        "for seq_index in [1200,1100,77,49,999]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
        "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
        "  print(\"예측문 :\",decoded_sentence[:-5])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 :  ▁피우 다 \n",
            "번역문 : ▁ タバコを ▁ 吸う \n",
            "예측문 :  ▁ 入れる\n",
            "\n",
            "\n",
            "원문 :  ▁아마 \n",
            "번역문 : ▁多分 \n",
            "예측문 :  ▁一緒に\n",
            "\n",
            "\n",
            "원문 :  ▁우리 ▁회사 에는 ▁일하 는 ▁사람이 ▁적다 ▁ . \n",
            "번역문 : ▁当 社 には 働き 手 が 少ない 。 \n",
            "예측문 :  ▁当 社 には 働き 手 が 少ない 。\n",
            "\n",
            "\n",
            "원문 :  ▁찾 다 \n",
            "번역문 : ▁探 す ▁ 見つけ る \n",
            "예측문 :  ▁もう ▁ 既に\n",
            "\n",
            "\n",
            "원문 :  ▁며칠 \n",
            "번역문 : ▁ 何日 ▁ 数日 \n",
            "예측문 :  ▁ ▁ 数日\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}